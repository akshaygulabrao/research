@misc{alamdariPersistentMonitoringDiscrete2012,
  title = {Persistent {{Monitoring}} in {{Discrete Environments}}: {{Minimizing}} the {{Maximum Weighted Latency Between Observations}}},
  shorttitle = {Persistent {{Monitoring}} in {{Discrete Environments}}},
  author = {Alamdari, Soroush and Fata, Elaheh and Smith, Stephen L.},
  year = {2012},
  month = oct,
  number = {arXiv:1202.5619},
  eprint = {1202.5619},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1202.5619},
  url = {http://arxiv.org/abs/1202.5619},
  urldate = {2024-12-03},
  abstract = {In this paper, we consider the problem of planning a path for a robot to monitor a known set of features of interest in an environment. We represent the environment as a graph with vertex weights and edge lengths. The vertices represent regions of interest, edge lengths give travel times between regions, and the vertex weights give the importance of each region. As the robot repeatedly performs a closed walk on the graph, we define the weighted latency of a vertex to be the maximum time between visits to that vertex, weighted by the importance (vertex weight) of that vertex. Our goal is to find a closed walk that minimizes the maximum weighted latency of any vertex. We show that there does not exist a polynomial time algorithm for the problem. We then provide two approximation algorithms; an \$O({\textbackslash}log n)\$-approximation algorithm and an \$O({\textbackslash}log {\textbackslash}rho\_G)\$-approximation algorithm, where \${\textbackslash}rho\_G\$ is the ratio between the maximum and minimum vertex weights. We provide simulation results which demonstrate that our algorithms can be applied to problems consisting of thousands of vertices, and a case study for patrolling a city for crime.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/Users/ox/Zotero/storage/CHYZ8GRQ/Alamdari et al. - 2012 - Persistent Monitoring in Discrete Environments Minimizing the Maximum Weighted Latency Between Obse.pdf;/Users/ox/Zotero/storage/CKNM364I/1202.html}
}

@book{atienzaAdvancedDeepLearning2020,
  title = {Advanced {{Deep Learning}} with {{TensorFlow}} 2 and {{Keras}}: {{Apply DL}}, {{GANs}}, {{VAEs}}, Deep {{RL}}, Unsupervised Learning, Object Detection and Segmentation, and More, 2nd {{Edition}}},
  shorttitle = {Advanced {{Deep Learning}} with {{TensorFlow}} 2 and {{Keras}}},
  author = {Atienza, Rowel},
  year = {2020},
  month = feb,
  edition = {2nd edition},
  publisher = {Packt Publishing},
  abstract = {Updated and revised second edition of the bestselling guide to advanced deep learning with TensorFlow 2 and KerasKey FeaturesExplore the most advanced deep learning techniques that drive modern AI resultsNew coverage of unsupervised deep learning using mutual information, object detection, and semantic segmentationCompletely updated for TensorFlow 2.xBook DescriptionAdvanced Deep Learning with TensorFlow 2 and Keras, Second Edition is a completely updated edition of the bestselling guide to the advanced deep learning techniques available today. Revised for TensorFlow 2.x, this edition introduces you to the practical side of deep learning with new chapters on unsupervised learning using mutual information, object detection (SSD), and semantic segmentation (FCN and PSPNet), further allowing you to create your own cutting-edge AI projects.Using Keras as an open-source deep learning library, the book features hands-on projects that show you how to create more effective AI with the most up-to-date techniques.Starting with an overview of multi-layer perceptrons (MLPs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs), the book then introduces more cutting-edge techniques as you explore deep neural network architectures, including ResNet and DenseNet, and how to create autoencoders. You will then learn about GANs, and how they can unlock new levels of AI performance.Next, you'll discover how a variational autoencoder (VAE) is implemented, and how GANs and VAEs have the generative power to synthesize data that can be extremely convincing to humans. You'll also learn to implement DRL such as Deep Q-Learning and Policy Gradient Methods, which are critical to many modern results in AI.What you will learnUse mutual information maximization techniques to perform unsupervised learningUse segmentation to identify the pixel-wise class of each object in an imageIdentify both the bounding box and class of objects in an image using object detectionLearn the building blocks for advanced techniques - MLPss, CNN, and RNNsUnderstand deep neural networks - including ResNet and DenseNetUnderstand and build autoregressive models -- autoencoders, VAEs, and GANsDiscover and implement deep reinforcement learning methodsWho this book is forThis is not an introductory book, so fluency with Python is required. The reader should also be familiar with some machine learning approaches, and practical experience with DL will also be helpful. Knowledge of Keras or TensorFlow 2.0 is not required but is recommended.Table of ContentsIntroducing Advanced Deep Learning with KerasDeep Neural NetworksAutoencodersGenerative Adversarial Networks (GANs)Improved GANsDisentangled Representation GANsCross-Domain GANsVariational Autoencoders (VAEs)Deep Reinforcement LearningPolicy Gradient MethodsObject DetectionSemantic SegmentationUnsupervised Learning Using Mutual Information},
  langid = {english}
}

@misc{atienzaAdvancedDeepLearningwithKerasChapter11detection2019,
  title = {Advanced-{{Deep-Learning-with-Keras}}/Chapter11-Detection},
  author = {Atienza, Rowel},
  year = {2019},
  journal = {Github},
  url = {https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/tree/master/chapter11-detection},
  urldate = {2024-12-02},
  file = {/Users/ox/Zotero/storage/7NZK4DIN/chapter11-detection.html}
}

@misc{chanMatthewachanVgraph2022,
  title = {Matthewachan/Vgraph},
  author = {Chan, Matthew},
  year = {2022},
  month = nov,
  url = {https://github.com/matthewachan/vgraph},
  urldate = {2024-12-03},
  abstract = {Visibility graph robot path planning in ROS},
  copyright = {GPL-3.0}
}

@inproceedings{deitsEfficientMixedintegerPlanning2015,
  title = {Efficient Mixed-Integer Planning for {{UAVs}} in Cluttered Environments},
  booktitle = {2015 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Deits, Robin and Tedrake, Russ},
  year = {2015},
  month = may,
  pages = {42--49},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/ICRA.2015.7138978},
  url = {http://ieeexplore.ieee.org/document/7138978/},
  urldate = {2024-12-02},
  abstract = {We present a new approach to the design of smooth trajectories for quadrotor unmanned aerial vehicles (UAVs), which are free of collisions with obstacles along their entire length. To avoid the non-convex constraints normally required for obstacle-avoidance, we perform a mixed-integer optimization in which polynomial trajectories are assigned to convex regions which are known to be obstacle-free. Prior approaches have used the faces of the obstacles themselves to define these convex regions. We instead use IRIS, a recently developed technique for greedy convex segmentation [1], to pre-compute convex regions of safe space. This results in a substantially reduced number of integer variables, which improves the speed with which the optimization can be solved to its global optimum, even for tens or hundreds of obstacle faces. In addition, prior approaches have typically enforced obstacle avoidance at a finite set of sample or knot points. We introduce a technique based on sums-of-squares (SOS) programming that allows us to ensure that the entire piecewise polynomial trajectory is free of collisions using convex constraints. We demonstrate this technique in 2D and in 3D using a dynamical model in the Drake toolbox for MATLAB [2].},
  isbn = {978-1-4799-6923-4},
  langid = {english},
  file = {/Users/ox/Zotero/storage/2DRQ8LJA/Deits and Tedrake - 2015 - Efficient mixed-integer planning for UAVs in cluttered environments.pdf}
}

@misc{erhanScalableObjectDetection2013,
  title = {Scalable {{Object Detection}} Using {{Deep Neural Networks}}},
  author = {Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir},
  year = {2013},
  month = dec,
  number = {arXiv:1312.2249},
  eprint = {1312.2249},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1312.2249},
  url = {http://arxiv.org/abs/1312.2249},
  urldate = {2024-12-03},
  abstract = {Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,OLD,Statistics - Machine Learning},
  file = {/Users/ox/Zotero/storage/DWYR2Q7V/Erhan et al. - 2013 - Scalable Object Detection using Deep Neural Networks.pdf;/Users/ox/Zotero/storage/7TS7GQCG/1312.html}
}

@inproceedings{haoDifferentialFlatnessbasedTrajectory2005,
  title = {Differential Flatness-Based Trajectory Planning for Multiple Unmanned Aerial Vehicles Using Mixed-Integer Linear Programming},
  booktitle = {Proceedings of the 2005, {{American Control Conference}}, 2005.},
  author = {Hao, Yongxing and Davari, A. and Manesh, A.},
  year = {2005},
  month = jun,
  pages = {104-109 vol. 1},
  issn = {2378-5861},
  doi = {10.1109/ACC.2005.1469916},
  url = {https://ieeexplore.ieee.org/document/1469916/?arnumber=1469916},
  urldate = {2024-12-04},
  abstract = {This paper provides a method for planning fuel-optimal trajectories for multiple unmanned aerial vehicles to reconfigure and traverse between goal points in a dynamic environment in real-time. Recent developments in robot motion planning have shown that trajectory optimization of linear vehicle systems including collision avoidance can be written as a linear program subject to mixed integer constraints, known as a mixed integer linear program (MILP). This paper extends the trajectory optimization to a class of nonlinear systems: differentially flat systems using MILP. A polynomial basis for a Ritz approximation of the optimal solution reduces the optimization variables and computation time without discretizing the systems. Based on the differential flatness property of unmanned vehicle systems, the trajectory planner satisfies the kinematic constraints of the individual vehicles while accounting for inter-vehicle collision and path constraints. The analytical fuel-optimal trajectories are smooth and continuous. Illustrative trajectory planning examples of multiple unmanned aerial vehicles are presented.},
  keywords = {Linear programming,Motion planning,Nonlinear systems,Polynomials,Remotely operated vehicles,Robot motion,Technology planning,Trajectory,Unmanned aerial vehicles,Vehicle dynamics},
  file = {/Users/ox/Zotero/storage/DNYPY2QI/1469916.html}
}

@article{hartFormalBasisHeuristic1968,
  title = {A {{Formal Basis}} for the {{Heuristic Determination}} of {{Minimum Cost Paths}}},
  author = {Hart, Peter E. and Nilsson, Nils J. and Raphael, Bertram},
  year = {1968},
  month = jul,
  journal = {IEEE Transactions on Systems Science and Cybernetics},
  volume = {4},
  number = {2},
  pages = {100--107},
  issn = {2168-2887},
  doi = {10.1109/TSSC.1968.300136},
  url = {https://ieeexplore.ieee.org/document/4082128},
  urldate = {2024-12-03},
  abstract = {Although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications, there has been no underlying theory to guide the development of efficient search procedures. Moreover, there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared. This paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of search strategies.},
  keywords = {Automatic control,Automatic programming,Chemical technology,Costs,Functional programming,Gradient methods,Instruction sets,Mathematical programming,Minimax techniques,Minimization methods},
  file = {/Users/ox/Zotero/storage/HHS845JH/4082128.html}
}

@misc{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = mar,
  number = {arXiv:1502.03167},
  eprint = {1502.03167},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1502.03167},
  url = {http://arxiv.org/abs/1502.03167},
  urldate = {2024-12-03},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/ox/Zotero/storage/D2KJATH2/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf;/Users/ox/Zotero/storage/BCWXMA42/1502.html}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  urldate = {2024-12-03},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file = {/Users/ox/Zotero/storage/GCFXFR5X/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf}
}

@book{lavallePlanningAlgorithms2006,
  title = {Planning {{Algorithms}}},
  author = {LaValle, Steven M.},
  year = {2006},
  month = may,
  edition = {1},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511546877},
  url = {https://www.cambridge.org/core/product/identifier/9780511546877/type/book},
  urldate = {2024-12-03},
  abstract = {Planning algorithms are impacting technical disciplines and industries around the world, including robotics, computer-aided design, manufacturing, computer graphics, aerospace applications, drug design, and protein folding. This coherent and comprehensive book unifies material from several sources, including robotics, control theory, artificial intelligence, and algorithms. The treatment is centered on robot motion planning, but integrates material on planning in discrete spaces. A major part of the book is devoted to planning under uncertainty, including decision theory, Markov decision processes, and information spaces, which are the 'configuration spaces' of all sensor-based planning problems. The last part of the book delves into planning under differential constraints that arise when automating the motions of virtually any mechanical system. This text and reference is intended for students, engineers, and researchers in robotics, artificial intelligence, and control theory as well as computer graphics, algorithms, and computational biology.},
  copyright = {https://www.cambridge.org/core/terms},
  isbn = {978-0-521-86205-9 978-0-511-54687-7},
  langid = {english},
  file = {/Users/ox/Zotero/storage/F54LWVZA/LaValle - 2006 - Planning Algorithms.pdf}
}

@misc{linFeaturePyramidNetworks2017,
  title = {Feature {{Pyramid Networks}} for {{Object Detection}}},
  author = {Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  year = {2017},
  month = apr,
  number = {arXiv:1612.03144},
  eprint = {1612.03144},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1612.03144},
  url = {http://arxiv.org/abs/1612.03144},
  urldate = {2024-12-02},
  abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ox/Zotero/storage/VQ72JNVR/Lin et al. - 2017 - Feature Pyramid Networks for Object Detection.pdf;/Users/ox/Zotero/storage/5DLAH4IU/1612.html}
}

@misc{liuSSDSingleShot2016,
  title = {{{SSD}}: {{Single Shot MultiBox Detector}}},
  shorttitle = {{{SSD}}},
  author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  year = {2016},
  month = dec,
  number = {arXiv:1512.02325},
  eprint = {1512.02325},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1512.02325},
  url = {http://arxiv.org/abs/1512.02325},
  urldate = {2024-12-02},
  abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For \$300{\textbackslash}times 300\$ input, SSD achieves 72.1\% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for \$500{\textbackslash}times 500\$ input, SSD achieves 75.1\% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ox/Zotero/storage/3K3QGZJT/Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf;/Users/ox/Zotero/storage/6XABIBRI/1512.html}
}

@article{lozano-perezAlgorithmPlanningCollisionfree1979,
  title = {An Algorithm for Planning Collision-Free Paths among Polyhedral Obstacles},
  author = {{Lozano-P{\'e}rez}, Tom{\'a}s and Wesley, Michael A.},
  year = {1979},
  month = oct,
  journal = {Communications of the ACM},
  volume = {22},
  number = {10},
  pages = {560--570},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/359156.359164},
  url = {https://dl.acm.org/doi/10.1145/359156.359164},
  urldate = {2024-12-03},
  langid = {english},
  file = {/Users/ox/Zotero/storage/ECIBHLRH/Lozano-Pérez and Wesley - 1979 - An algorithm for planning collision-free paths among polyhedral obstacles.pdf}
}

@misc{marcucciMotionPlanningObstacles2022,
  title = {Motion {{Planning}} around {{Obstacles}} with {{Convex Optimization}}},
  author = {Marcucci, Tobia and Petersen, Mark and von Wrangel, David and Tedrake, Russ},
  year = {2022},
  month = may,
  number = {arXiv:2205.04422},
  eprint = {2205.04422},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.04422},
  url = {http://arxiv.org/abs/2205.04422},
  urldate = {2024-12-04},
  abstract = {Trajectory optimization offers mature tools for motion planning in high-dimensional spaces under dynamic constraints. However, when facing complex configuration spaces, cluttered with obstacles, roboticists typically fall back to sampling-based planners that struggle in very high dimensions and with continuous differential constraints. Indeed, obstacles are the source of many textbook examples of problematic nonconvexities in the trajectory-optimization problem. Here we show that convex optimization can, in fact, be used to reliably plan trajectories around obstacles. Specifically, we consider planning problems with collision-avoidance constraints, as well as cost penalties and hard constraints on the shape, the duration, and the velocity of the trajectory. Combining the properties of B{\textbackslash}'ezier curves with a recently-proposed framework for finding shortest paths in Graphs of Convex Sets (GCS), we formulate the planning problem as a compact mixed-integer optimization. In stark contrast with existing mixed-integer planners, the convex relaxation of our programs is very tight, and a cheap rounding of its solution is typically sufficient to design globally-optimal trajectories. This reduces the mixed-integer program back to a simple convex optimization, and automatically provides optimality bounds for the planned trajectories. We name the proposed planner GCS, after its underlying optimization framework. We demonstrate GCS in simulation on a variety of robotic platforms, including a quadrotor flying through buildings and a dual-arm manipulator (with fourteen degrees of freedom) moving in a confined space. Using numerical experiments on a seven-degree-of-freedom manipulator, we show that GCS can outperform widely-used sampling-based planners by finding higher-quality trajectories in less time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {/Users/ox/Zotero/storage/6TM4K3K4/Marcucci et al. - 2022 - Motion Planning around Obstacles with Convex Optimization.pdf;/Users/ox/Zotero/storage/9WSKSZER/2205.html}
}

@misc{MathZettlrUser,
  title = {Math - {{Zettlr User Manual}}},
  url = {https://docs.zettlr.com/en/core/math/},
  urldate = {2024-12-02},
  file = {/Users/ox/Zotero/storage/6A2IZQ87/math.html}
}

@misc{metaTorchonnxDocumentation,
  title = {Torch.Onnx --- {{Documentation}}},
  author = {Meta},
  journal = {torch},
  url = {https://pytorch.org/docs/stable/onnx.html},
  urldate = {2024-12-03},
  file = {/Users/ox/Zotero/storage/H3KTCZN4/onnx.html}
}

@misc{NVIDIATensorRT2024,
  title = {{{NVIDIA}}/{{TensorRT}}},
  year = {2024},
  month = dec,
  url = {https://github.com/NVIDIA/TensorRT},
  urldate = {2024-12-03},
  abstract = {NVIDIA{\textregistered} TensorRT™ is an SDK for high-performance deep learning inference on NVIDIA GPUs. This repository contains the open source components of TensorRT.},
  copyright = {Apache-2.0},
  howpublished = {NVIDIA Corporation},
  keywords = {deep-learning,gpu-acceleration,inference,nvidia,tensorrt}
}

@misc{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2015},
  month = apr,
  number = {arXiv:1409.1556},
  eprint = {1409.1556},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1409.1556},
  url = {http://arxiv.org/abs/1409.1556},
  urldate = {2024-12-03},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ox/Zotero/storage/6X79GSNM/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf;/Users/ox/Zotero/storage/7L2P476T/1409.html}
}

@misc{Tensorflowonnx2024,
  title = {Tensorflow-Onnx},
  year = {2024},
  month = nov,
  url = {https://github.com/onnx/tensorflow-onnx},
  urldate = {2024-12-03},
  abstract = {Convert TensorFlow, Keras, Tensorflow.js and Tflite models to ONNX},
  copyright = {Apache-2.0},
  howpublished = {Open Source},
  keywords = {convert,deep-learning,export,keras,onnx,tensorflow,tensorflowjs,tflite}
}

@misc{TensorRTPluginBatchedNMSPlugin,
  title = {{{TensorRT}}/Plugin/{{batchedNMSPlugin}} at Release/10.6 {$\cdot$} {{NVIDIA}}/{{TensorRT}}},
  url = {https://github.com/NVIDIA/TensorRT/tree/release/10.6/plugin/batchedNMSPlugin},
  urldate = {2024-12-03},
  langid = {english},
  file = {/Users/ox/Zotero/storage/3LPDI5SC/batchedNMSPlugin.html}
}

@misc{tower10ipaHiOneYJust2014,
  type = {Reddit {{Post}}},
  title = {Hi {{OneY}}. {{I}} Just Don't like the Company of Other Men.. i'd Rather Have Female Friends},
  author = {{tower10ipa}},
  year = {2014},
  month = dec,
  journal = {r/OneY},
  url = {www.reddit.com/r/OneY/comments/2qd77l/hi_oney_i_just_dont_like_the_company_of_other_men/},
  urldate = {2024-12-03},
  file = {/Users/ox/Zotero/storage/JBAK2XJ8/hi_oney_i_just_dont_like_the_company_of_other_men.html}
}

@misc{TransformingAugmentingImages,
  title = {Transforming and Augmenting Images --- {{Torchvision}} 0.20 Documentation},
  url = {https://pytorch.org/vision/0.20/transforms.html#v2-api-reference-recommended},
  urldate = {2024-12-02},
  file = {/Users/ox/Zotero/storage/FR6R6KS2/transforms.html}
}

@misc{UsingPymavlinkLibraries,
  title = {Using {{Pymavlink Libraries}}},
  journal = {Mavlink},
  url = {https://mavlink.io/en/mavgen_python/},
  urldate = {2024-12-03},
  file = {/Users/ox/Zotero/storage/WALM4JXH/mavgen_python.html}
}

@misc{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2024-12-02},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ox/Zotero/storage/MY6GLNFM/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/Users/ox/Zotero/storage/Z45TET7B/1706.html}
}

@misc{zhangVarifocalNetIoUawareDense2021,
  title = {{{VarifocalNet}}: {{An IoU-aware Dense Object Detector}}},
  shorttitle = {{{VarifocalNet}}},
  author = {Zhang, Haoyang and Wang, Ying and Dayoub, Feras and S{\"u}nderhauf, Niko},
  year = {2021},
  month = mar,
  number = {arXiv:2008.13367},
  eprint = {2008.13367},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2008.13367},
  url = {http://arxiv.org/abs/2008.13367},
  urldate = {2024-12-03},
  abstract = {Accurately ranking the vast number of candidate detections is crucial for dense object detectors to achieve high performance. Prior work uses the classification score or a combination of classification and predicted localization scores to rank candidates. However, neither option results in a reliable ranking, thus degrading detection performance. In this paper, we propose to learn an Iou-aware Classification Score (IACS) as a joint representation of object presence confidence and localization accuracy. We show that dense object detectors can achieve a more accurate ranking of candidate detections based on the IACS. We design a new loss function, named Varifocal Loss, to train a dense object detector to predict the IACS, and propose a new star-shaped bounding box feature representation for IACS prediction and bounding box refinement. Combining these two new components and a bounding box refinement branch, we build an IoU-aware dense object detector based on the FCOS+ATSS architecture, that we call VarifocalNet or VFNet for short. Extensive experiments on MS COCO show that our VFNet consistently surpasses the strong baseline by \${\textbackslash}sim\$2.0 AP with different backbones. Our best model VFNet-X-1200 with Res2Net-101-DCN achieves a single-model single-scale AP of 55.1 on COCO test-dev, which is state-of-the-art among various object detectors.Code is available at https://github.com/hyz-xmaster/VarifocalNet .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ox/Zotero/storage/IBP88DFI/Zhang et al. - 2021 - VarifocalNet An IoU-aware Dense Object Detector.pdf;/Users/ox/Zotero/storage/DGT4UVUV/2008.html}
}

@misc{zhaoDETRsBeatYOLOs2024,
  title = {{{DETRs Beat YOLOs}} on {{Real-time Object Detection}}},
  author = {Zhao, Yian and Lv, Wenyu and Xu, Shangliang and Wei, Jinman and Wang, Guanzhong and Dang, Qingqing and Liu, Yi and Chen, Jie},
  year = {2024},
  month = apr,
  number = {arXiv:2304.08069},
  eprint = {2304.08069},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.08069},
  url = {http://arxiv.org/abs/2304.08069},
  urldate = {2024-12-02},
  abstract = {The YOLO series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. However, we observe that the speed and accuracy of YOLOs are negatively affected by the NMS. Recently, end-to-end Transformer-based detectors (DETRs) have provided an alternative to eliminating NMS. Nevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS. In this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. We build RT-DETR in two steps, drawing on the advanced DETR: first we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. Specifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. Then, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy. In addition, RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. Our RT-DETR-R50 / R101 achieves 53.1\% / 54.3\% AP on COCO and 108 / 74 FPS on T4 GPU, outperforming previously advanced YOLOs in both speed and accuracy. We also develop scaled RT-DETRs that outperform the lighter YOLO detectors (S and M models). Furthermore, RT-DETR-R50 outperforms DINO-R50 by 2.2\% AP in accuracy and about 21 times in FPS. After pre-training with Objects365, RT-DETR-R50 / R101 achieves 55.3\% / 56.2\% AP. The project page: https://zhao-yian.github.io/RTDETR.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ox/Zotero/storage/K2D7K86T/Zhao et al. - 2024 - DETRs Beat YOLOs on Real-time Object Detection.pdf;/Users/ox/Zotero/storage/KBDPDT45/2304.html}
}
